{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd20b95b",
   "metadata": {},
   "source": [
    "# 6.4.1 오버피팅\n",
    "- 매개변수가 많고 표현력이 높은 모델\n",
    "- 훈련 데이터가 적음\n",
    "- weight_decay_lambda = 0\n",
    "\n",
    "# 6.4.2 가중치 감소 (weight decay)\n",
    "- weight_decay_lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3200a022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.06666666666666667, test acc:0.0867\n",
      "epoch:1, train acc:0.07666666666666666, test acc:0.0999\n",
      "epoch:2, train acc:0.10333333333333333, test acc:0.1115\n",
      "epoch:3, train acc:0.12666666666666668, test acc:0.125\n",
      "epoch:4, train acc:0.16666666666666666, test acc:0.1381\n",
      "epoch:5, train acc:0.20666666666666667, test acc:0.1538\n",
      "epoch:6, train acc:0.24, test acc:0.1627\n",
      "epoch:7, train acc:0.2733333333333333, test acc:0.1788\n",
      "epoch:8, train acc:0.31333333333333335, test acc:0.1988\n",
      "epoch:9, train acc:0.32666666666666666, test acc:0.2155\n",
      "epoch:10, train acc:0.36666666666666664, test acc:0.2411\n",
      "epoch:11, train acc:0.42, test acc:0.2661\n",
      "epoch:12, train acc:0.44666666666666666, test acc:0.2794\n",
      "epoch:13, train acc:0.5033333333333333, test acc:0.3144\n",
      "epoch:14, train acc:0.5533333333333333, test acc:0.3586\n",
      "epoch:15, train acc:0.6033333333333334, test acc:0.4041\n",
      "epoch:16, train acc:0.6366666666666667, test acc:0.4434\n",
      "epoch:17, train acc:0.66, test acc:0.4682\n",
      "epoch:18, train acc:0.7133333333333334, test acc:0.4995\n",
      "epoch:19, train acc:0.7166666666666667, test acc:0.5106\n",
      "epoch:20, train acc:0.72, test acc:0.5156\n",
      "epoch:21, train acc:0.74, test acc:0.5318\n",
      "epoch:22, train acc:0.7466666666666667, test acc:0.5467\n",
      "epoch:23, train acc:0.7566666666666667, test acc:0.5506\n",
      "epoch:24, train acc:0.7533333333333333, test acc:0.5625\n",
      "epoch:25, train acc:0.7566666666666667, test acc:0.5515\n",
      "epoch:26, train acc:0.7633333333333333, test acc:0.5745\n",
      "epoch:27, train acc:0.7733333333333333, test acc:0.5851\n",
      "epoch:28, train acc:0.7833333333333333, test acc:0.5877\n",
      "epoch:29, train acc:0.7833333333333333, test acc:0.5903\n",
      "epoch:30, train acc:0.7833333333333333, test acc:0.5987\n",
      "epoch:31, train acc:0.79, test acc:0.6023\n",
      "epoch:32, train acc:0.79, test acc:0.6157\n",
      "epoch:33, train acc:0.81, test acc:0.6165\n",
      "epoch:34, train acc:0.82, test acc:0.6285\n",
      "epoch:35, train acc:0.8266666666666667, test acc:0.623\n",
      "epoch:36, train acc:0.82, test acc:0.6376\n",
      "epoch:37, train acc:0.8266666666666667, test acc:0.6303\n",
      "epoch:38, train acc:0.83, test acc:0.6368\n",
      "epoch:39, train acc:0.8333333333333334, test acc:0.6503\n",
      "epoch:40, train acc:0.8466666666666667, test acc:0.6557\n",
      "epoch:41, train acc:0.8566666666666667, test acc:0.6487\n",
      "epoch:42, train acc:0.8666666666666667, test acc:0.66\n",
      "epoch:43, train acc:0.88, test acc:0.6752\n",
      "epoch:44, train acc:0.8833333333333333, test acc:0.672\n",
      "epoch:45, train acc:0.8733333333333333, test acc:0.6809\n",
      "epoch:46, train acc:0.89, test acc:0.6802\n",
      "epoch:47, train acc:0.89, test acc:0.682\n",
      "epoch:48, train acc:0.9033333333333333, test acc:0.69\n",
      "epoch:49, train acc:0.8966666666666666, test acc:0.6707\n",
      "epoch:50, train acc:0.9133333333333333, test acc:0.6885\n",
      "epoch:51, train acc:0.9266666666666666, test acc:0.696\n",
      "epoch:52, train acc:0.9233333333333333, test acc:0.7011\n",
      "epoch:53, train acc:0.93, test acc:0.7068\n",
      "epoch:54, train acc:0.9133333333333333, test acc:0.705\n",
      "epoch:55, train acc:0.9333333333333333, test acc:0.6931\n",
      "epoch:56, train acc:0.9333333333333333, test acc:0.7128\n",
      "epoch:57, train acc:0.94, test acc:0.709\n",
      "epoch:58, train acc:0.95, test acc:0.7094\n",
      "epoch:59, train acc:0.9466666666666667, test acc:0.7107\n",
      "epoch:60, train acc:0.9466666666666667, test acc:0.7098\n",
      "epoch:61, train acc:0.9633333333333334, test acc:0.7199\n",
      "epoch:62, train acc:0.9533333333333334, test acc:0.714\n",
      "epoch:63, train acc:0.9566666666666667, test acc:0.7228\n",
      "epoch:64, train acc:0.9666666666666667, test acc:0.7241\n",
      "epoch:65, train acc:0.9633333333333334, test acc:0.7199\n",
      "epoch:66, train acc:0.97, test acc:0.7248\n",
      "epoch:67, train acc:0.98, test acc:0.7201\n",
      "epoch:68, train acc:0.97, test acc:0.7259\n",
      "epoch:69, train acc:0.9766666666666667, test acc:0.7212\n",
      "epoch:70, train acc:0.9666666666666667, test acc:0.7253\n",
      "epoch:71, train acc:0.9666666666666667, test acc:0.7247\n",
      "epoch:72, train acc:0.9766666666666667, test acc:0.7311\n",
      "epoch:73, train acc:0.98, test acc:0.731\n",
      "epoch:74, train acc:0.9766666666666667, test acc:0.7313\n",
      "epoch:75, train acc:0.98, test acc:0.732\n",
      "epoch:76, train acc:0.9733333333333334, test acc:0.7353\n",
      "epoch:77, train acc:0.98, test acc:0.7324\n",
      "epoch:78, train acc:0.9766666666666667, test acc:0.734\n",
      "epoch:79, train acc:0.9733333333333334, test acc:0.7326\n",
      "epoch:80, train acc:0.99, test acc:0.7396\n",
      "epoch:81, train acc:0.9833333333333333, test acc:0.7359\n",
      "epoch:82, train acc:0.99, test acc:0.7414\n",
      "epoch:83, train acc:0.9866666666666667, test acc:0.7407\n",
      "epoch:84, train acc:0.9833333333333333, test acc:0.7423\n",
      "epoch:85, train acc:0.9866666666666667, test acc:0.7404\n",
      "epoch:86, train acc:0.99, test acc:0.7411\n",
      "epoch:87, train acc:0.9933333333333333, test acc:0.7411\n",
      "epoch:88, train acc:0.99, test acc:0.7399\n",
      "epoch:89, train acc:0.9866666666666667, test acc:0.7325\n",
      "epoch:90, train acc:0.99, test acc:0.7461\n",
      "epoch:91, train acc:0.99, test acc:0.744\n",
      "epoch:92, train acc:0.9933333333333333, test acc:0.7401\n",
      "epoch:93, train acc:0.9933333333333333, test acc:0.7397\n",
      "epoch:94, train acc:0.9933333333333333, test acc:0.7457\n",
      "epoch:95, train acc:0.9933333333333333, test acc:0.7453\n",
      "epoch:96, train acc:0.9933333333333333, test acc:0.7429\n",
      "epoch:97, train acc:0.9933333333333333, test acc:0.7478\n",
      "epoch:98, train acc:0.9933333333333333, test acc:0.745\n",
      "epoch:99, train acc:0.9933333333333333, test acc:0.7395\n",
      "epoch:100, train acc:0.9933333333333333, test acc:0.7452\n",
      "epoch:101, train acc:0.9933333333333333, test acc:0.7492\n",
      "epoch:102, train acc:0.9933333333333333, test acc:0.7524\n",
      "epoch:103, train acc:0.9966666666666667, test acc:0.751\n",
      "epoch:104, train acc:0.9933333333333333, test acc:0.7507\n",
      "epoch:105, train acc:0.9966666666666667, test acc:0.7498\n",
      "epoch:106, train acc:0.9966666666666667, test acc:0.7477\n",
      "epoch:107, train acc:0.9966666666666667, test acc:0.7517\n",
      "epoch:108, train acc:0.9966666666666667, test acc:0.7523\n",
      "epoch:109, train acc:0.9966666666666667, test acc:0.7493\n",
      "epoch:110, train acc:0.9966666666666667, test acc:0.7496\n",
      "epoch:111, train acc:0.9966666666666667, test acc:0.7474\n",
      "epoch:112, train acc:1.0, test acc:0.7471\n",
      "epoch:113, train acc:1.0, test acc:0.7506\n",
      "epoch:114, train acc:0.9966666666666667, test acc:0.7478\n",
      "epoch:115, train acc:1.0, test acc:0.7515\n",
      "epoch:116, train acc:1.0, test acc:0.7499\n",
      "epoch:117, train acc:1.0, test acc:0.754\n",
      "epoch:118, train acc:1.0, test acc:0.7536\n",
      "epoch:119, train acc:1.0, test acc:0.7593\n",
      "epoch:120, train acc:1.0, test acc:0.7548\n",
      "epoch:121, train acc:1.0, test acc:0.7533\n",
      "epoch:122, train acc:1.0, test acc:0.7533\n",
      "epoch:123, train acc:1.0, test acc:0.7553\n",
      "epoch:124, train acc:1.0, test acc:0.7563\n",
      "epoch:125, train acc:1.0, test acc:0.7504\n",
      "epoch:126, train acc:1.0, test acc:0.7554\n",
      "epoch:127, train acc:1.0, test acc:0.7565\n",
      "epoch:128, train acc:1.0, test acc:0.7559\n",
      "epoch:129, train acc:1.0, test acc:0.7569\n",
      "epoch:130, train acc:1.0, test acc:0.7567\n",
      "epoch:131, train acc:1.0, test acc:0.7585\n",
      "epoch:132, train acc:1.0, test acc:0.7558\n",
      "epoch:133, train acc:1.0, test acc:0.7584\n",
      "epoch:134, train acc:1.0, test acc:0.7557\n",
      "epoch:135, train acc:1.0, test acc:0.7582\n",
      "epoch:136, train acc:1.0, test acc:0.7541\n",
      "epoch:137, train acc:1.0, test acc:0.7598\n",
      "epoch:138, train acc:1.0, test acc:0.7539\n",
      "epoch:139, train acc:1.0, test acc:0.7572\n",
      "epoch:140, train acc:1.0, test acc:0.7501\n",
      "epoch:141, train acc:1.0, test acc:0.7514\n",
      "epoch:142, train acc:1.0, test acc:0.7538\n",
      "epoch:143, train acc:1.0, test acc:0.7586\n",
      "epoch:144, train acc:1.0, test acc:0.757\n",
      "epoch:145, train acc:1.0, test acc:0.7555\n",
      "epoch:146, train acc:1.0, test acc:0.7563\n",
      "epoch:147, train acc:1.0, test acc:0.7589\n",
      "epoch:148, train acc:1.0, test acc:0.7589\n",
      "epoch:149, train acc:1.0, test acc:0.7606\n",
      "epoch:150, train acc:1.0, test acc:0.7572\n",
      "epoch:151, train acc:1.0, test acc:0.7573\n",
      "epoch:152, train acc:1.0, test acc:0.7574\n",
      "epoch:153, train acc:1.0, test acc:0.7572\n",
      "epoch:154, train acc:1.0, test acc:0.7579\n",
      "epoch:155, train acc:1.0, test acc:0.7544\n",
      "epoch:156, train acc:1.0, test acc:0.7584\n",
      "epoch:157, train acc:1.0, test acc:0.7575\n",
      "epoch:158, train acc:1.0, test acc:0.757\n",
      "epoch:159, train acc:1.0, test acc:0.7609\n",
      "epoch:160, train acc:1.0, test acc:0.7591\n",
      "epoch:161, train acc:1.0, test acc:0.7587\n",
      "epoch:162, train acc:1.0, test acc:0.7579\n",
      "epoch:163, train acc:1.0, test acc:0.7595\n",
      "epoch:164, train acc:1.0, test acc:0.7578\n",
      "epoch:165, train acc:1.0, test acc:0.7549\n",
      "epoch:166, train acc:1.0, test acc:0.7561\n",
      "epoch:167, train acc:1.0, test acc:0.7589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:168, train acc:1.0, test acc:0.7587\n",
      "epoch:169, train acc:1.0, test acc:0.7583\n",
      "epoch:170, train acc:1.0, test acc:0.7574\n",
      "epoch:171, train acc:1.0, test acc:0.7592\n",
      "epoch:172, train acc:1.0, test acc:0.7545\n",
      "epoch:173, train acc:1.0, test acc:0.7573\n",
      "epoch:174, train acc:1.0, test acc:0.7571\n",
      "epoch:175, train acc:1.0, test acc:0.7595\n",
      "epoch:176, train acc:1.0, test acc:0.7604\n",
      "epoch:177, train acc:1.0, test acc:0.7605\n",
      "epoch:178, train acc:1.0, test acc:0.7585\n",
      "epoch:179, train acc:1.0, test acc:0.7588\n",
      "epoch:180, train acc:1.0, test acc:0.7583\n",
      "epoch:181, train acc:1.0, test acc:0.7604\n",
      "epoch:182, train acc:1.0, test acc:0.7603\n",
      "epoch:183, train acc:1.0, test acc:0.7608\n",
      "epoch:184, train acc:1.0, test acc:0.7591\n",
      "epoch:185, train acc:1.0, test acc:0.7615\n",
      "epoch:186, train acc:1.0, test acc:0.7619\n",
      "epoch:187, train acc:1.0, test acc:0.7594\n",
      "epoch:188, train acc:1.0, test acc:0.7619\n",
      "epoch:189, train acc:1.0, test acc:0.7599\n",
      "epoch:190, train acc:1.0, test acc:0.7616\n",
      "epoch:191, train acc:1.0, test acc:0.7611\n",
      "epoch:192, train acc:1.0, test acc:0.7614\n",
      "epoch:193, train acc:1.0, test acc:0.7619\n",
      "epoch:194, train acc:1.0, test acc:0.759\n",
      "epoch:195, train acc:1.0, test acc:0.7604\n",
      "epoch:196, train acc:1.0, test acc:0.7597\n",
      "epoch:197, train acc:1.0, test acc:0.7621\n",
      "epoch:198, train acc:1.0, test acc:0.7621\n",
      "epoch:199, train acc:1.0, test acc:0.7607\n",
      "epoch:200, train acc:1.0, test acc:0.7624\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_learning_github.dataset.mnist import load_mnist\n",
    "from deep_learning_github.common.multi_layer_net import MultiLayerNet\n",
    "from deep_learning_github.common.optimizer import SGD\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# weight decay（가중치 감쇠） 설정 =======================\n",
    "weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
    "# weight_decay_lambda = 0.1\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e896f",
   "metadata": {},
   "source": [
    "# 6.4.3 드롭아웃(Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a5751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3b49740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.30921676863729\n",
      "=== epoch:1, train acc:0.09333333333333334, test acc:0.0954 ===\n",
      "train loss:2.298861365949211\n",
      "train loss:2.3132623226239932\n",
      "train loss:2.301742696501429\n",
      "=== epoch:2, train acc:0.09666666666666666, test acc:0.0957 ===\n",
      "train loss:2.3262331918226336\n",
      "train loss:2.323829108439069\n",
      "train loss:2.3016775804521394\n",
      "=== epoch:3, train acc:0.09666666666666666, test acc:0.0953 ===\n",
      "train loss:2.3054348075764564\n",
      "train loss:2.3063456897385906\n",
      "train loss:2.294141622807897\n",
      "=== epoch:4, train acc:0.09666666666666666, test acc:0.0956 ===\n",
      "train loss:2.3192409448795117\n",
      "train loss:2.3020345963002455\n",
      "train loss:2.299535507216578\n",
      "=== epoch:5, train acc:0.09333333333333334, test acc:0.0959 ===\n",
      "train loss:2.3105042630828936\n",
      "train loss:2.3138665070060065\n",
      "train loss:2.307426263432346\n",
      "=== epoch:6, train acc:0.09, test acc:0.0958 ===\n",
      "train loss:2.3042543722738413\n",
      "train loss:2.3103742652663737\n",
      "train loss:2.3110030609426206\n",
      "=== epoch:7, train acc:0.09333333333333334, test acc:0.0959 ===\n",
      "train loss:2.2931623936173695\n",
      "train loss:2.302826045426279\n",
      "train loss:2.294612024444473\n",
      "=== epoch:8, train acc:0.09, test acc:0.0959 ===\n",
      "train loss:2.296737142601089\n",
      "train loss:2.293344779680271\n",
      "train loss:2.2936492462968174\n",
      "=== epoch:9, train acc:0.09, test acc:0.096 ===\n",
      "train loss:2.288685225162215\n",
      "train loss:2.303568403034282\n",
      "train loss:2.288500384819197\n",
      "=== epoch:10, train acc:0.09, test acc:0.0967 ===\n",
      "train loss:2.2780002167732207\n",
      "train loss:2.288073263695028\n",
      "train loss:2.301018315373248\n",
      "=== epoch:11, train acc:0.09666666666666666, test acc:0.0976 ===\n",
      "train loss:2.308024861544402\n",
      "train loss:2.3209056731980584\n",
      "train loss:2.2989879207154122\n",
      "=== epoch:12, train acc:0.1, test acc:0.0982 ===\n",
      "train loss:2.2832056412697916\n",
      "train loss:2.3064138914551138\n",
      "train loss:2.285532827042112\n",
      "=== epoch:13, train acc:0.1, test acc:0.0995 ===\n",
      "train loss:2.2919497297974285\n",
      "train loss:2.2936377296590806\n",
      "train loss:2.287449830096926\n",
      "=== epoch:14, train acc:0.1, test acc:0.1007 ===\n",
      "train loss:2.3043055893176243\n",
      "train loss:2.289757821927596\n",
      "train loss:2.2952851557120457\n",
      "=== epoch:15, train acc:0.10333333333333333, test acc:0.1007 ===\n",
      "train loss:2.2929919762931013\n",
      "train loss:2.2824030836569515\n",
      "train loss:2.2802908828414403\n",
      "=== epoch:16, train acc:0.1, test acc:0.1027 ===\n",
      "train loss:2.2781218330127424\n",
      "train loss:2.295877277547041\n",
      "train loss:2.2981287126826357\n",
      "=== epoch:17, train acc:0.10333333333333333, test acc:0.104 ===\n",
      "train loss:2.283779743268733\n",
      "train loss:2.2815254378448606\n",
      "train loss:2.28435727997089\n",
      "=== epoch:18, train acc:0.10666666666666667, test acc:0.105 ===\n",
      "train loss:2.281786139900334\n",
      "train loss:2.2852222319566895\n",
      "train loss:2.2880038532262623\n",
      "=== epoch:19, train acc:0.11333333333333333, test acc:0.1069 ===\n",
      "train loss:2.293348030737269\n",
      "train loss:2.2892217737193206\n",
      "train loss:2.307877235817865\n",
      "=== epoch:20, train acc:0.12, test acc:0.1087 ===\n",
      "train loss:2.2949745450772134\n",
      "train loss:2.2924116429432093\n",
      "train loss:2.2760407484990752\n",
      "=== epoch:21, train acc:0.13666666666666666, test acc:0.1136 ===\n",
      "train loss:2.28567533907152\n",
      "train loss:2.288361923117813\n",
      "train loss:2.2835281396858624\n",
      "=== epoch:22, train acc:0.14, test acc:0.1153 ===\n",
      "train loss:2.2921116296383217\n",
      "train loss:2.2775473207105064\n",
      "train loss:2.2904094330501166\n",
      "=== epoch:23, train acc:0.15333333333333332, test acc:0.1224 ===\n",
      "train loss:2.2798204039572996\n",
      "train loss:2.28973187426985\n",
      "train loss:2.2624904389791656\n",
      "=== epoch:24, train acc:0.15, test acc:0.1272 ===\n",
      "train loss:2.274084022284057\n",
      "train loss:2.2700353754166374\n",
      "train loss:2.2608197044584957\n",
      "=== epoch:25, train acc:0.14666666666666667, test acc:0.1281 ===\n",
      "train loss:2.273092777036409\n",
      "train loss:2.275623332994301\n",
      "train loss:2.2732382228342733\n",
      "=== epoch:26, train acc:0.15666666666666668, test acc:0.1305 ===\n",
      "train loss:2.2739016502851475\n",
      "train loss:2.2747981792791783\n",
      "train loss:2.278212664749633\n",
      "=== epoch:27, train acc:0.16, test acc:0.1346 ===\n",
      "train loss:2.2804750316671307\n",
      "train loss:2.2725534426367684\n",
      "train loss:2.2840519478451307\n",
      "=== epoch:28, train acc:0.16, test acc:0.1367 ===\n",
      "train loss:2.268686394107144\n",
      "train loss:2.279434636150955\n",
      "train loss:2.2836646581869036\n",
      "=== epoch:29, train acc:0.18, test acc:0.1405 ===\n",
      "train loss:2.2694788576919733\n",
      "train loss:2.2829246214346304\n",
      "train loss:2.262427314228284\n",
      "=== epoch:30, train acc:0.17666666666666667, test acc:0.1426 ===\n",
      "train loss:2.2782617769694014\n",
      "train loss:2.272778663024517\n",
      "train loss:2.2728927620330985\n",
      "=== epoch:31, train acc:0.18666666666666668, test acc:0.1489 ===\n",
      "train loss:2.2606602283316355\n",
      "train loss:2.2590871064487654\n",
      "train loss:2.2744203329250627\n",
      "=== epoch:32, train acc:0.18, test acc:0.1475 ===\n",
      "train loss:2.273687891363282\n",
      "train loss:2.272582632619209\n",
      "train loss:2.2661316508358964\n",
      "=== epoch:33, train acc:0.18666666666666668, test acc:0.1493 ===\n",
      "train loss:2.2654895164900095\n",
      "train loss:2.2826927997438937\n",
      "train loss:2.2866311926157503\n",
      "=== epoch:34, train acc:0.20666666666666667, test acc:0.1578 ===\n",
      "train loss:2.2618489147559013\n",
      "train loss:2.2724974699476617\n",
      "train loss:2.2596890070278253\n",
      "=== epoch:35, train acc:0.21333333333333335, test acc:0.1645 ===\n",
      "train loss:2.265098760283401\n",
      "train loss:2.2722980447022874\n",
      "train loss:2.259860012534182\n",
      "=== epoch:36, train acc:0.23, test acc:0.1728 ===\n",
      "train loss:2.26276492350139\n",
      "train loss:2.2664846195398023\n",
      "train loss:2.267061446257147\n",
      "=== epoch:37, train acc:0.24, test acc:0.1756 ===\n",
      "train loss:2.253337457872513\n",
      "train loss:2.257361177962116\n",
      "train loss:2.256938500553031\n",
      "=== epoch:38, train acc:0.25333333333333335, test acc:0.1781 ===\n",
      "train loss:2.2471917703880937\n",
      "train loss:2.25499840760483\n",
      "train loss:2.256308672234704\n",
      "=== epoch:39, train acc:0.25666666666666665, test acc:0.1805 ===\n",
      "train loss:2.2567648603004646\n",
      "train loss:2.262254538675416\n",
      "train loss:2.25554876011411\n",
      "=== epoch:40, train acc:0.2633333333333333, test acc:0.1864 ===\n",
      "train loss:2.2561305922272608\n",
      "train loss:2.2492356920903243\n",
      "train loss:2.262729641645687\n",
      "=== epoch:41, train acc:0.26666666666666666, test acc:0.1902 ===\n",
      "train loss:2.2234231668532094\n",
      "train loss:2.247102016141793\n",
      "train loss:2.2575294525520717\n",
      "=== epoch:42, train acc:0.26, test acc:0.1902 ===\n",
      "train loss:2.2522956958353535\n",
      "train loss:2.248267638769743\n",
      "train loss:2.239432009035822\n",
      "=== epoch:43, train acc:0.2733333333333333, test acc:0.1925 ===\n",
      "train loss:2.2600170465254648\n",
      "train loss:2.2435005689709366\n",
      "train loss:2.239289799998933\n",
      "=== epoch:44, train acc:0.27666666666666667, test acc:0.2015 ===\n",
      "train loss:2.2561023794572024\n",
      "train loss:2.2437600296171474\n",
      "train loss:2.244021042880093\n",
      "=== epoch:45, train acc:0.28, test acc:0.2083 ===\n",
      "train loss:2.234324950535814\n",
      "train loss:2.242421717701745\n",
      "train loss:2.2562887311321633\n",
      "=== epoch:46, train acc:0.2833333333333333, test acc:0.2103 ===\n",
      "train loss:2.246788438420856\n",
      "train loss:2.247557225883305\n",
      "train loss:2.2406886487203077\n",
      "=== epoch:47, train acc:0.2833333333333333, test acc:0.2126 ===\n",
      "train loss:2.243593548661201\n",
      "train loss:2.2624762154588085\n",
      "train loss:2.2484996761836222\n",
      "=== epoch:48, train acc:0.29, test acc:0.2229 ===\n",
      "train loss:2.2375500454932937\n",
      "train loss:2.248531165718614\n",
      "train loss:2.244560490499368\n",
      "=== epoch:49, train acc:0.31, test acc:0.2306 ===\n",
      "train loss:2.2474483433295727\n",
      "train loss:2.230208922554251\n",
      "train loss:2.2426349722529415\n",
      "=== epoch:50, train acc:0.30666666666666664, test acc:0.2303 ===\n",
      "train loss:2.2395016134873726\n",
      "train loss:2.234270324052936\n",
      "train loss:2.2421730596186893\n",
      "=== epoch:51, train acc:0.32, test acc:0.236 ===\n",
      "train loss:2.232937694578546\n",
      "train loss:2.2426119030800433\n",
      "train loss:2.2616388457848084\n",
      "=== epoch:52, train acc:0.3233333333333333, test acc:0.2402 ===\n",
      "train loss:2.2275919980654675\n",
      "train loss:2.2587987311161863\n",
      "train loss:2.224831097953432\n",
      "=== epoch:53, train acc:0.32, test acc:0.2498 ===\n",
      "train loss:2.2254339103596967\n",
      "train loss:2.238967572776719\n",
      "train loss:2.2376709194609066\n",
      "=== epoch:54, train acc:0.31333333333333335, test acc:0.2543 ===\n",
      "train loss:2.2457349330279066\n",
      "train loss:2.2309403898442834\n",
      "train loss:2.2345276931503846\n",
      "=== epoch:55, train acc:0.3233333333333333, test acc:0.257 ===\n",
      "train loss:2.2381669860499436\n",
      "train loss:2.2212187761591373\n",
      "train loss:2.2213800919396207\n",
      "=== epoch:56, train acc:0.32666666666666666, test acc:0.259 ===\n",
      "train loss:2.234139704059452\n",
      "train loss:2.233653099580035\n",
      "train loss:2.2287432427377714\n",
      "=== epoch:57, train acc:0.31333333333333335, test acc:0.2656 ===\n",
      "train loss:2.2459907722908024\n",
      "train loss:2.236266441425286\n",
      "train loss:2.2277979570026925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:58, train acc:0.32, test acc:0.2687 ===\n",
      "train loss:2.2400731769726145\n",
      "train loss:2.224880794436797\n",
      "train loss:2.240922917056774\n",
      "=== epoch:59, train acc:0.31, test acc:0.2685 ===\n",
      "train loss:2.2093450025609647\n",
      "train loss:2.226754814012354\n",
      "train loss:2.222053536913765\n",
      "=== epoch:60, train acc:0.33, test acc:0.2679 ===\n",
      "train loss:2.2142498495077727\n",
      "train loss:2.225585130005549\n",
      "train loss:2.1975561108750834\n",
      "=== epoch:61, train acc:0.3233333333333333, test acc:0.2691 ===\n",
      "train loss:2.2298466785742783\n",
      "train loss:2.2360960347079457\n",
      "train loss:2.2081679654455018\n",
      "=== epoch:62, train acc:0.32666666666666666, test acc:0.2721 ===\n",
      "train loss:2.1974172287274145\n",
      "train loss:2.19908756865734\n",
      "train loss:2.2245794057906094\n",
      "=== epoch:63, train acc:0.33, test acc:0.2726 ===\n",
      "train loss:2.2335581527855983\n",
      "train loss:2.2062301652687237\n",
      "train loss:2.203082891850903\n",
      "=== epoch:64, train acc:0.3233333333333333, test acc:0.274 ===\n",
      "train loss:2.2105683901057605\n",
      "train loss:2.2083296694079824\n",
      "train loss:2.207346218867512\n",
      "=== epoch:65, train acc:0.3233333333333333, test acc:0.2736 ===\n",
      "train loss:2.2180850915441073\n",
      "train loss:2.2148912877387334\n",
      "train loss:2.202215152956657\n",
      "=== epoch:66, train acc:0.3333333333333333, test acc:0.2765 ===\n",
      "train loss:2.2013138124243605\n",
      "train loss:2.2073889310581776\n",
      "train loss:2.202894593819418\n",
      "=== epoch:67, train acc:0.32666666666666666, test acc:0.2765 ===\n",
      "train loss:2.222854605474494\n",
      "train loss:2.1978783230717864\n",
      "train loss:2.199992128350136\n",
      "=== epoch:68, train acc:0.32, test acc:0.2777 ===\n",
      "train loss:2.1888255499749003\n",
      "train loss:2.1955376832778146\n",
      "train loss:2.192441864023994\n",
      "=== epoch:69, train acc:0.32666666666666666, test acc:0.2744 ===\n",
      "train loss:2.2113271654890174\n",
      "train loss:2.178110676726618\n",
      "train loss:2.157188456058693\n",
      "=== epoch:70, train acc:0.32, test acc:0.2731 ===\n",
      "train loss:2.2144402153677487\n",
      "train loss:2.187847161618638\n",
      "train loss:2.187574449346794\n",
      "=== epoch:71, train acc:0.31333333333333335, test acc:0.2724 ===\n",
      "train loss:2.198199704190914\n",
      "train loss:2.1984440603391717\n",
      "train loss:2.1823454639112474\n",
      "=== epoch:72, train acc:0.31333333333333335, test acc:0.2729 ===\n",
      "train loss:2.1906463150043796\n",
      "train loss:2.199755423759393\n",
      "train loss:2.1999165622687364\n",
      "=== epoch:73, train acc:0.31333333333333335, test acc:0.2746 ===\n",
      "train loss:2.1986972287708926\n",
      "train loss:2.2007794368672027\n",
      "train loss:2.1995559140279384\n",
      "=== epoch:74, train acc:0.31333333333333335, test acc:0.274 ===\n",
      "train loss:2.1731247689222974\n",
      "train loss:2.1599373433437488\n",
      "train loss:2.173084999959184\n",
      "=== epoch:75, train acc:0.31, test acc:0.2745 ===\n",
      "train loss:2.1753276114115354\n",
      "train loss:2.1884262813817985\n",
      "train loss:2.1882956460253804\n",
      "=== epoch:76, train acc:0.32, test acc:0.2766 ===\n",
      "train loss:2.1976920824429165\n",
      "train loss:2.1996891895751336\n",
      "train loss:2.184979557298842\n",
      "=== epoch:77, train acc:0.32, test acc:0.2764 ===\n",
      "train loss:2.1942389847045654\n",
      "train loss:2.1934637798094987\n",
      "train loss:2.175892761999738\n",
      "=== epoch:78, train acc:0.32666666666666666, test acc:0.2782 ===\n",
      "train loss:2.189398435099636\n",
      "train loss:2.1704991498442734\n",
      "train loss:2.14900266739209\n",
      "=== epoch:79, train acc:0.32666666666666666, test acc:0.2787 ===\n",
      "train loss:2.162792098829811\n",
      "train loss:2.164717298875432\n",
      "train loss:2.1801296724476935\n",
      "=== epoch:80, train acc:0.32666666666666666, test acc:0.2785 ===\n",
      "train loss:2.187181297055712\n",
      "train loss:2.2009750106836194\n",
      "train loss:2.1615979325387564\n",
      "=== epoch:81, train acc:0.33, test acc:0.2829 ===\n",
      "train loss:2.15672150343754\n",
      "train loss:2.2100923527200815\n",
      "train loss:2.188581424978611\n",
      "=== epoch:82, train acc:0.34, test acc:0.2865 ===\n",
      "train loss:2.1753722436879017\n",
      "train loss:2.1895633080841432\n",
      "train loss:2.1693308224575336\n",
      "=== epoch:83, train acc:0.3433333333333333, test acc:0.2874 ===\n",
      "train loss:2.177589513600355\n",
      "train loss:2.1800835961926928\n",
      "train loss:2.156878108917766\n",
      "=== epoch:84, train acc:0.3466666666666667, test acc:0.2905 ===\n",
      "train loss:2.156579656651545\n",
      "train loss:2.1719344434855317\n",
      "train loss:2.177337845353002\n",
      "=== epoch:85, train acc:0.35, test acc:0.2923 ===\n",
      "train loss:2.1377085392816713\n",
      "train loss:2.139238160660289\n",
      "train loss:2.1630658361192032\n",
      "=== epoch:86, train acc:0.3433333333333333, test acc:0.2911 ===\n",
      "train loss:2.1682806477531105\n",
      "train loss:2.1650203468284097\n",
      "train loss:2.15800623151315\n",
      "=== epoch:87, train acc:0.35, test acc:0.2943 ===\n",
      "train loss:2.1331376991641844\n",
      "train loss:2.1652785593051096\n",
      "train loss:2.1332806565844806\n",
      "=== epoch:88, train acc:0.3466666666666667, test acc:0.2935 ===\n",
      "train loss:2.1516989967452984\n",
      "train loss:2.1313107277056274\n",
      "train loss:2.154678980334464\n",
      "=== epoch:89, train acc:0.3433333333333333, test acc:0.2902 ===\n",
      "train loss:2.139055426791137\n",
      "train loss:2.166607508519692\n",
      "train loss:2.1499169589316622\n",
      "=== epoch:90, train acc:0.35, test acc:0.2919 ===\n",
      "train loss:2.0939441937804815\n",
      "train loss:2.132299778432758\n",
      "train loss:2.135447330273779\n",
      "=== epoch:91, train acc:0.3466666666666667, test acc:0.2939 ===\n",
      "train loss:2.1748470947501057\n",
      "train loss:2.1322984757933128\n",
      "train loss:2.1552737543585754\n",
      "=== epoch:92, train acc:0.3433333333333333, test acc:0.2941 ===\n",
      "train loss:2.142418785946908\n",
      "train loss:2.144414723110299\n",
      "train loss:2.13218879862551\n",
      "=== epoch:93, train acc:0.3466666666666667, test acc:0.2992 ===\n",
      "train loss:2.115158009494358\n",
      "train loss:2.1216826007163263\n",
      "train loss:2.1544728622335447\n",
      "=== epoch:94, train acc:0.3466666666666667, test acc:0.2986 ===\n",
      "train loss:2.1646627831074694\n",
      "train loss:2.143350218774903\n",
      "train loss:2.1064719560948966\n",
      "=== epoch:95, train acc:0.35, test acc:0.2969 ===\n",
      "train loss:2.1247343750933227\n",
      "train loss:2.145406516999546\n",
      "train loss:2.1059532396289837\n",
      "=== epoch:96, train acc:0.3466666666666667, test acc:0.2989 ===\n",
      "train loss:2.151833934562218\n",
      "train loss:2.159759742483031\n",
      "train loss:2.0759762105007478\n",
      "=== epoch:97, train acc:0.3466666666666667, test acc:0.3009 ===\n",
      "train loss:2.1171422512503297\n",
      "train loss:2.1454258731567064\n",
      "train loss:2.1585298363123666\n",
      "=== epoch:98, train acc:0.3433333333333333, test acc:0.296 ===\n",
      "train loss:2.1234677193578273\n",
      "train loss:2.1152664220243698\n",
      "train loss:2.0763198919459263\n",
      "=== epoch:99, train acc:0.3433333333333333, test acc:0.2981 ===\n",
      "train loss:2.0982296437619614\n",
      "train loss:2.105992516889687\n",
      "train loss:2.1196472779901203\n",
      "=== epoch:100, train acc:0.35, test acc:0.2992 ===\n",
      "train loss:2.099474818115898\n",
      "train loss:2.112431683178806\n",
      "train loss:2.0948974603140655\n",
      "=== epoch:101, train acc:0.3466666666666667, test acc:0.2965 ===\n",
      "train loss:2.085720233817317\n",
      "train loss:2.088359291084714\n",
      "train loss:2.1200795536936705\n",
      "=== epoch:102, train acc:0.3433333333333333, test acc:0.2944 ===\n",
      "train loss:2.049263752868251\n",
      "train loss:2.0749645516120085\n",
      "train loss:2.1131132393241985\n",
      "=== epoch:103, train acc:0.3333333333333333, test acc:0.2888 ===\n",
      "train loss:2.0439545008310964\n",
      "train loss:2.0816277335206834\n",
      "train loss:2.107268697580484\n",
      "=== epoch:104, train acc:0.3233333333333333, test acc:0.2875 ===\n",
      "train loss:2.0382952203190197\n",
      "train loss:2.113267113433039\n",
      "train loss:2.1012751755308927\n",
      "=== epoch:105, train acc:0.32666666666666666, test acc:0.2906 ===\n",
      "train loss:2.1051194490110223\n",
      "train loss:2.1268627900444326\n",
      "train loss:2.0943038737893946\n",
      "=== epoch:106, train acc:0.32666666666666666, test acc:0.2882 ===\n",
      "train loss:2.0978772034251136\n",
      "train loss:2.085624109426962\n",
      "train loss:2.037957787944686\n",
      "=== epoch:107, train acc:0.3233333333333333, test acc:0.283 ===\n",
      "train loss:2.1171525928695396\n",
      "train loss:2.04528636898918\n",
      "train loss:2.083648840733642\n",
      "=== epoch:108, train acc:0.32666666666666666, test acc:0.2839 ===\n",
      "train loss:2.047189977352454\n",
      "train loss:2.1275021769694087\n",
      "train loss:2.049984664072619\n",
      "=== epoch:109, train acc:0.32666666666666666, test acc:0.2814 ===\n",
      "train loss:2.062747065114588\n",
      "train loss:2.07449015006228\n",
      "train loss:2.0805149971363197\n",
      "=== epoch:110, train acc:0.32666666666666666, test acc:0.2821 ===\n",
      "train loss:2.0050035076048425\n",
      "train loss:2.1292366031475476\n",
      "train loss:2.1343764530599723\n",
      "=== epoch:111, train acc:0.33, test acc:0.2818 ===\n",
      "train loss:1.9933904859926759\n",
      "train loss:2.0558760189331293\n",
      "train loss:2.0446292015697285\n",
      "=== epoch:112, train acc:0.32666666666666666, test acc:0.2792 ===\n",
      "train loss:2.0461655724264425\n",
      "train loss:2.044332943584247\n",
      "train loss:2.029205696209351\n",
      "=== epoch:113, train acc:0.32666666666666666, test acc:0.2821 ===\n",
      "train loss:2.0502760511831304\n",
      "train loss:2.1256599491406014\n",
      "train loss:2.0706533388550947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:114, train acc:0.33, test acc:0.2824 ===\n",
      "train loss:1.9588074208110557\n",
      "train loss:2.0299144224823333\n",
      "train loss:2.0799805990116536\n",
      "=== epoch:115, train acc:0.33, test acc:0.2809 ===\n",
      "train loss:2.0601616648007672\n",
      "train loss:2.0275750770678727\n",
      "train loss:2.054475697054026\n",
      "=== epoch:116, train acc:0.33, test acc:0.2845 ===\n",
      "train loss:2.0236467771673854\n",
      "train loss:2.093433184287094\n",
      "train loss:2.040234731121665\n",
      "=== epoch:117, train acc:0.34, test acc:0.2864 ===\n",
      "train loss:1.985172793449751\n",
      "train loss:2.011777025127353\n",
      "train loss:1.980043692755664\n",
      "=== epoch:118, train acc:0.3333333333333333, test acc:0.2891 ===\n",
      "train loss:2.0226683000958356\n",
      "train loss:1.9908990579327612\n",
      "train loss:2.0363582784510537\n",
      "=== epoch:119, train acc:0.34, test acc:0.2898 ===\n",
      "train loss:2.074993276676513\n",
      "train loss:2.0053275795937995\n",
      "train loss:2.037683571341407\n",
      "=== epoch:120, train acc:0.3433333333333333, test acc:0.2907 ===\n",
      "train loss:2.013513338318691\n",
      "train loss:2.020563679934943\n",
      "train loss:1.9876517161791218\n",
      "=== epoch:121, train acc:0.35333333333333333, test acc:0.2939 ===\n",
      "train loss:2.045929257529011\n",
      "train loss:2.0057528164684237\n",
      "train loss:2.0369237530623323\n",
      "=== epoch:122, train acc:0.35333333333333333, test acc:0.2964 ===\n",
      "train loss:1.9446058375947652\n",
      "train loss:2.0281352494362035\n",
      "train loss:2.0038004658820823\n",
      "=== epoch:123, train acc:0.35333333333333333, test acc:0.294 ===\n",
      "train loss:1.9278148911051816\n",
      "train loss:1.9726100764745553\n",
      "train loss:2.0145300320437123\n",
      "=== epoch:124, train acc:0.35333333333333333, test acc:0.293 ===\n",
      "train loss:1.9484993123451526\n",
      "train loss:1.9684407065599665\n",
      "train loss:1.9625725504747547\n",
      "=== epoch:125, train acc:0.35333333333333333, test acc:0.2918 ===\n",
      "train loss:1.9239394530527993\n",
      "train loss:2.0169565256368633\n",
      "train loss:2.0591943755314746\n",
      "=== epoch:126, train acc:0.36, test acc:0.2966 ===\n",
      "train loss:1.9346582875178044\n",
      "train loss:1.9390551205666444\n",
      "train loss:2.0439234990732214\n",
      "=== epoch:127, train acc:0.37, test acc:0.2967 ===\n",
      "train loss:1.9873238676663647\n",
      "train loss:1.9727835191657326\n",
      "train loss:2.0170280982513487\n",
      "=== epoch:128, train acc:0.37, test acc:0.2968 ===\n",
      "train loss:1.9683722174638043\n",
      "train loss:1.93049042455652\n",
      "train loss:1.9801483117002514\n",
      "=== epoch:129, train acc:0.37333333333333335, test acc:0.2964 ===\n",
      "train loss:2.0088857356090624\n",
      "train loss:1.9945087731401512\n",
      "train loss:1.891681278085573\n",
      "=== epoch:130, train acc:0.37666666666666665, test acc:0.2979 ===\n",
      "train loss:1.9695540315902322\n",
      "train loss:1.8902189439120456\n",
      "train loss:1.9839186188331297\n",
      "=== epoch:131, train acc:0.36, test acc:0.2972 ===\n",
      "train loss:1.8587357629156351\n",
      "train loss:1.9742800923760542\n",
      "train loss:1.9492125999544405\n",
      "=== epoch:132, train acc:0.37, test acc:0.2998 ===\n",
      "train loss:1.9310453398744363\n",
      "train loss:1.9164069682821034\n",
      "train loss:2.00189962117699\n",
      "=== epoch:133, train acc:0.3566666666666667, test acc:0.2981 ===\n",
      "train loss:2.000961365773352\n",
      "train loss:1.9811140979671378\n",
      "train loss:2.01504585213581\n",
      "=== epoch:134, train acc:0.37, test acc:0.2988 ===\n",
      "train loss:1.899747276925476\n",
      "train loss:1.860895841609461\n",
      "train loss:1.973473736662151\n",
      "=== epoch:135, train acc:0.36333333333333334, test acc:0.2989 ===\n",
      "train loss:1.875111818140699\n",
      "train loss:1.9797184407437578\n",
      "train loss:1.9865905200442495\n",
      "=== epoch:136, train acc:0.37, test acc:0.3023 ===\n",
      "train loss:1.9672768572138757\n",
      "train loss:1.9668457111337774\n",
      "train loss:1.9449016561157666\n",
      "=== epoch:137, train acc:0.36666666666666664, test acc:0.2996 ===\n",
      "train loss:1.865158536477272\n",
      "train loss:2.0352079995665635\n",
      "train loss:1.978379468077832\n",
      "=== epoch:138, train acc:0.36333333333333334, test acc:0.299 ===\n",
      "train loss:1.9265013863628133\n",
      "train loss:1.906980575778783\n",
      "train loss:2.0024691063826263\n",
      "=== epoch:139, train acc:0.37, test acc:0.3002 ===\n",
      "train loss:1.943795106955466\n",
      "train loss:1.974062739171749\n",
      "train loss:1.9290739560053618\n",
      "=== epoch:140, train acc:0.37333333333333335, test acc:0.3024 ===\n",
      "train loss:1.9411445336172601\n",
      "train loss:1.8878687932286158\n",
      "train loss:1.9332545147118037\n",
      "=== epoch:141, train acc:0.37333333333333335, test acc:0.3009 ===\n",
      "train loss:1.95201593614071\n",
      "train loss:1.8957999624594712\n",
      "train loss:1.9401386113590373\n",
      "=== epoch:142, train acc:0.37666666666666665, test acc:0.3045 ===\n",
      "train loss:1.8828659984207303\n",
      "train loss:1.9140495074503832\n",
      "train loss:1.9041953803181166\n",
      "=== epoch:143, train acc:0.37666666666666665, test acc:0.3032 ===\n",
      "train loss:1.9408040212725262\n",
      "train loss:1.932257880051024\n",
      "train loss:1.9150355617900694\n",
      "=== epoch:144, train acc:0.37, test acc:0.2975 ===\n",
      "train loss:1.9312816853812265\n",
      "train loss:1.8890234954241019\n",
      "train loss:1.9471753743224864\n",
      "=== epoch:145, train acc:0.37666666666666665, test acc:0.2959 ===\n",
      "train loss:1.8884096268843964\n",
      "train loss:1.910359711994281\n",
      "train loss:1.889129786653522\n",
      "=== epoch:146, train acc:0.37, test acc:0.2922 ===\n",
      "train loss:1.8487046481004163\n",
      "train loss:1.9271477647580988\n",
      "train loss:1.7603690017536098\n",
      "=== epoch:147, train acc:0.37, test acc:0.2905 ===\n",
      "train loss:1.7834511777359585\n",
      "train loss:1.8856415402405617\n",
      "train loss:1.9096393055717984\n",
      "=== epoch:148, train acc:0.36666666666666664, test acc:0.2893 ===\n",
      "train loss:1.8529180395514095\n",
      "train loss:1.9057337336064735\n",
      "train loss:1.9233206553792481\n",
      "=== epoch:149, train acc:0.36666666666666664, test acc:0.2948 ===\n",
      "train loss:1.8500348687856538\n",
      "train loss:1.8259325712126857\n",
      "train loss:1.7854847097363729\n",
      "=== epoch:150, train acc:0.38333333333333336, test acc:0.2974 ===\n",
      "train loss:1.9165832455190863\n",
      "train loss:1.891924198479499\n",
      "train loss:1.8513659291949096\n",
      "=== epoch:151, train acc:0.37333333333333335, test acc:0.2983 ===\n",
      "train loss:1.9774047715594065\n",
      "train loss:1.7903457691581903\n",
      "train loss:1.9154900238233226\n",
      "=== epoch:152, train acc:0.38333333333333336, test acc:0.3006 ===\n",
      "train loss:1.770322003893874\n",
      "train loss:1.8112994835706926\n",
      "train loss:1.8074364424410234\n",
      "=== epoch:153, train acc:0.39666666666666667, test acc:0.3042 ===\n",
      "train loss:1.8521439836888585\n",
      "train loss:1.8704469768625842\n",
      "train loss:1.7928853770392745\n",
      "=== epoch:154, train acc:0.38666666666666666, test acc:0.3052 ===\n",
      "train loss:1.8919255505318637\n",
      "train loss:1.8366001465785873\n",
      "train loss:1.8350378688827251\n",
      "=== epoch:155, train acc:0.3933333333333333, test acc:0.3119 ===\n",
      "train loss:1.7693029795598596\n",
      "train loss:1.859425395657654\n",
      "train loss:1.9682478673774362\n",
      "=== epoch:156, train acc:0.39, test acc:0.3152 ===\n",
      "train loss:1.93205914940344\n",
      "train loss:1.865306196044016\n",
      "train loss:1.896635889104909\n",
      "=== epoch:157, train acc:0.39666666666666667, test acc:0.3169 ===\n",
      "train loss:1.780324580315896\n",
      "train loss:1.8366457110619039\n",
      "train loss:1.8739462676422314\n",
      "=== epoch:158, train acc:0.39666666666666667, test acc:0.3198 ===\n",
      "train loss:1.8969286374498966\n",
      "train loss:1.9013421201916518\n",
      "train loss:1.9029413585556687\n",
      "=== epoch:159, train acc:0.4166666666666667, test acc:0.3269 ===\n",
      "train loss:1.9734629444676317\n",
      "train loss:1.8409419173681103\n",
      "train loss:1.8855647957478725\n",
      "=== epoch:160, train acc:0.4166666666666667, test acc:0.3279 ===\n",
      "train loss:1.835776253040248\n",
      "train loss:1.8595216478229182\n",
      "train loss:1.7832584744782682\n",
      "=== epoch:161, train acc:0.43666666666666665, test acc:0.3341 ===\n",
      "train loss:1.876006064412622\n",
      "train loss:1.8324447249701792\n",
      "train loss:1.8139815706226385\n",
      "=== epoch:162, train acc:0.43666666666666665, test acc:0.3365 ===\n",
      "train loss:1.8115742892446456\n",
      "train loss:1.7834651827210846\n",
      "train loss:1.7454993677019432\n",
      "=== epoch:163, train acc:0.42333333333333334, test acc:0.334 ===\n",
      "train loss:1.856907185464267\n",
      "train loss:1.773006757151207\n",
      "train loss:1.7405767709175575\n",
      "=== epoch:164, train acc:0.43333333333333335, test acc:0.3375 ===\n",
      "train loss:1.7734305313219818\n",
      "train loss:1.788856797527187\n",
      "train loss:1.8497669666728658\n",
      "=== epoch:165, train acc:0.43333333333333335, test acc:0.3379 ===\n",
      "train loss:1.8125967163945864\n",
      "train loss:1.7812795852429568\n",
      "train loss:1.7284095048006782\n",
      "=== epoch:166, train acc:0.44333333333333336, test acc:0.3439 ===\n",
      "train loss:1.7901517304184376\n",
      "train loss:1.6968911649403196\n",
      "train loss:1.7412007367405264\n",
      "=== epoch:167, train acc:0.44, test acc:0.3405 ===\n",
      "train loss:1.7639569660859105\n",
      "train loss:1.883045209519565\n",
      "train loss:1.7851312157200885\n",
      "=== epoch:168, train acc:0.44666666666666666, test acc:0.3451 ===\n",
      "train loss:1.787158825668095\n",
      "train loss:1.8149882325161313\n",
      "train loss:1.858720525820365\n",
      "=== epoch:169, train acc:0.46, test acc:0.3534 ===\n",
      "train loss:1.75838998088337\n",
      "train loss:1.7580090788744698\n",
      "train loss:1.814066181200094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:170, train acc:0.4633333333333333, test acc:0.3588 ===\n",
      "train loss:1.7479734992944913\n",
      "train loss:1.6896015721399538\n",
      "train loss:1.73914056849772\n",
      "=== epoch:171, train acc:0.46, test acc:0.3579 ===\n",
      "train loss:1.7140757960858317\n",
      "train loss:1.8143927036800518\n",
      "train loss:1.8033200076575153\n",
      "=== epoch:172, train acc:0.46, test acc:0.3591 ===\n",
      "train loss:1.8712497238838812\n",
      "train loss:1.794641977428332\n",
      "train loss:1.6536775221793139\n",
      "=== epoch:173, train acc:0.46, test acc:0.3582 ===\n",
      "train loss:1.8469132462955216\n",
      "train loss:1.7882182408968053\n",
      "train loss:1.5513901686068836\n",
      "=== epoch:174, train acc:0.45666666666666667, test acc:0.3531 ===\n",
      "train loss:1.6701377740842849\n",
      "train loss:1.7841792523670748\n",
      "train loss:1.8050679325928596\n",
      "=== epoch:175, train acc:0.46, test acc:0.3573 ===\n",
      "train loss:1.675124714788903\n",
      "train loss:1.6498464330403178\n",
      "train loss:1.6374013180817644\n",
      "=== epoch:176, train acc:0.46, test acc:0.3571 ===\n",
      "train loss:1.7773559450417402\n",
      "train loss:1.764919807120735\n",
      "train loss:1.7490697161151343\n",
      "=== epoch:177, train acc:0.4633333333333333, test acc:0.3605 ===\n",
      "train loss:1.7344609962280058\n",
      "train loss:1.762013026949604\n",
      "train loss:1.8279747675052271\n",
      "=== epoch:178, train acc:0.4633333333333333, test acc:0.3614 ===\n",
      "train loss:1.7248053944329325\n",
      "train loss:1.737264233909107\n",
      "train loss:1.685519654304743\n",
      "=== epoch:179, train acc:0.4633333333333333, test acc:0.3574 ===\n",
      "train loss:1.6200150079917301\n",
      "train loss:1.7155870722820779\n",
      "train loss:1.6532074242086388\n",
      "=== epoch:180, train acc:0.47, test acc:0.3606 ===\n",
      "train loss:1.6058667880335467\n",
      "train loss:1.6672934700221569\n",
      "train loss:1.7077130248892893\n",
      "=== epoch:181, train acc:0.4866666666666667, test acc:0.3671 ===\n",
      "train loss:1.7237215120620266\n",
      "train loss:1.6500447746116447\n",
      "train loss:1.7241244212762166\n",
      "=== epoch:182, train acc:0.49333333333333335, test acc:0.3753 ===\n",
      "train loss:1.758519236556278\n",
      "train loss:1.7865667824867355\n",
      "train loss:1.687671150759418\n",
      "=== epoch:183, train acc:0.5066666666666667, test acc:0.3843 ===\n",
      "train loss:1.6610028387381055\n",
      "train loss:1.6752222836269466\n",
      "train loss:1.7188614912458848\n",
      "=== epoch:184, train acc:0.5066666666666667, test acc:0.3853 ===\n",
      "train loss:1.6156091553836354\n",
      "train loss:1.61888157505901\n",
      "train loss:1.6558866196703008\n",
      "=== epoch:185, train acc:0.5066666666666667, test acc:0.3824 ===\n",
      "train loss:1.6644824244783718\n",
      "train loss:1.6634621708849904\n",
      "train loss:1.6478419643040363\n",
      "=== epoch:186, train acc:0.5033333333333333, test acc:0.3898 ===\n",
      "train loss:1.7718513020619988\n",
      "train loss:1.6234686358598973\n",
      "train loss:1.7274824158749498\n",
      "=== epoch:187, train acc:0.5066666666666667, test acc:0.3898 ===\n",
      "train loss:1.693740432045275\n",
      "train loss:1.7125813930305682\n",
      "train loss:1.6509710220700011\n",
      "=== epoch:188, train acc:0.5, test acc:0.3858 ===\n",
      "train loss:1.7341664805534398\n",
      "train loss:1.6431061397546616\n",
      "train loss:1.6281122667281207\n",
      "=== epoch:189, train acc:0.5033333333333333, test acc:0.3952 ===\n",
      "train loss:1.548563788992877\n",
      "train loss:1.6028463476505868\n",
      "train loss:1.7569985057792437\n",
      "=== epoch:190, train acc:0.5033333333333333, test acc:0.3978 ===\n",
      "train loss:1.6598354016228627\n",
      "train loss:1.608034021584578\n",
      "train loss:1.6735344733712383\n",
      "=== epoch:191, train acc:0.5133333333333333, test acc:0.4031 ===\n",
      "train loss:1.7323369489776865\n",
      "train loss:1.605770345880852\n",
      "train loss:1.5903947958060543\n",
      "=== epoch:192, train acc:0.5166666666666667, test acc:0.3997 ===\n",
      "train loss:1.6019834942469446\n",
      "train loss:1.545870644859105\n",
      "train loss:1.7039732517323176\n",
      "=== epoch:193, train acc:0.5066666666666667, test acc:0.4016 ===\n",
      "train loss:1.6102645233348363\n",
      "train loss:1.6168233709846966\n",
      "train loss:1.7028184486256759\n",
      "=== epoch:194, train acc:0.5166666666666667, test acc:0.4019 ===\n",
      "train loss:1.6192084704244387\n",
      "train loss:1.6624670475823486\n",
      "train loss:1.6276280704762323\n",
      "=== epoch:195, train acc:0.5166666666666667, test acc:0.4066 ===\n",
      "train loss:1.6519655418420691\n",
      "train loss:1.6531800712942544\n",
      "train loss:1.6965801220071848\n",
      "=== epoch:196, train acc:0.5166666666666667, test acc:0.4049 ===\n",
      "train loss:1.6631918092380737\n",
      "train loss:1.6092690849148696\n",
      "train loss:1.5768969730696798\n",
      "=== epoch:197, train acc:0.5333333333333333, test acc:0.4111 ===\n",
      "train loss:1.6436169498029372\n",
      "train loss:1.6566376289142948\n",
      "train loss:1.7449275414990282\n",
      "=== epoch:198, train acc:0.5333333333333333, test acc:0.4137 ===\n",
      "train loss:1.6579037090356445\n",
      "train loss:1.6592178201924697\n",
      "train loss:1.741529717891183\n",
      "=== epoch:199, train acc:0.55, test acc:0.4224 ===\n",
      "train loss:1.522450350940612\n",
      "train loss:1.6321628836684454\n",
      "train loss:1.615409694419036\n",
      "=== epoch:200, train acc:0.5366666666666666, test acc:0.4204 ===\n",
      "train loss:1.527326167048801\n",
      "train loss:1.5816735719894508\n",
      "train loss:1.6823515132390248\n",
      "=== epoch:201, train acc:0.5366666666666666, test acc:0.4249 ===\n",
      "train loss:1.5681297397378093\n",
      "train loss:1.5273084662381435\n",
      "train loss:1.5625280615110517\n",
      "=== epoch:202, train acc:0.54, test acc:0.427 ===\n",
      "train loss:1.6373251200719323\n",
      "train loss:1.6346223997322802\n",
      "train loss:1.5713654617396846\n",
      "=== epoch:203, train acc:0.5466666666666666, test acc:0.4266 ===\n",
      "train loss:1.5800670899119995\n",
      "train loss:1.6056917979121053\n",
      "train loss:1.622189747430735\n",
      "=== epoch:204, train acc:0.5466666666666666, test acc:0.4289 ===\n",
      "train loss:1.5135299480268258\n",
      "train loss:1.5012869924110401\n",
      "train loss:1.5607508826503946\n",
      "=== epoch:205, train acc:0.5333333333333333, test acc:0.4244 ===\n",
      "train loss:1.560618961910114\n",
      "train loss:1.4997365201765773\n",
      "train loss:1.6021551045111682\n",
      "=== epoch:206, train acc:0.5366666666666666, test acc:0.4167 ===\n",
      "train loss:1.5166091164725701\n",
      "train loss:1.4965566949240736\n",
      "train loss:1.5601500296559836\n",
      "=== epoch:207, train acc:0.5366666666666666, test acc:0.4177 ===\n",
      "train loss:1.5861485679659715\n",
      "train loss:1.601282182180666\n",
      "train loss:1.5172961989292557\n",
      "=== epoch:208, train acc:0.5466666666666666, test acc:0.426 ===\n",
      "train loss:1.5223723785470553\n",
      "train loss:1.5574131628078323\n",
      "train loss:1.532784041909129\n",
      "=== epoch:209, train acc:0.5566666666666666, test acc:0.4278 ===\n",
      "train loss:1.533309996655058\n",
      "train loss:1.5247227733662398\n",
      "train loss:1.5589026132545143\n",
      "=== epoch:210, train acc:0.5466666666666666, test acc:0.4279 ===\n",
      "train loss:1.5356070893765967\n",
      "train loss:1.5367092566412308\n",
      "train loss:1.5754447875462523\n",
      "=== epoch:211, train acc:0.5666666666666667, test acc:0.43 ===\n",
      "train loss:1.3841573227840323\n",
      "train loss:1.494694570389902\n",
      "train loss:1.5506694091806836\n",
      "=== epoch:212, train acc:0.56, test acc:0.4292 ===\n",
      "train loss:1.4028260445791576\n",
      "train loss:1.4723444804422956\n",
      "train loss:1.5286427132679556\n",
      "=== epoch:213, train acc:0.56, test acc:0.4353 ===\n",
      "train loss:1.4697196767950123\n",
      "train loss:1.4078690982288438\n",
      "train loss:1.5827051202970066\n",
      "=== epoch:214, train acc:0.5533333333333333, test acc:0.4336 ===\n",
      "train loss:1.5493109940706153\n",
      "train loss:1.5202556522966892\n",
      "train loss:1.4779051167613746\n",
      "=== epoch:215, train acc:0.5633333333333334, test acc:0.4441 ===\n",
      "train loss:1.5244797946970459\n",
      "train loss:1.5209941697409095\n",
      "train loss:1.5331062091975547\n",
      "=== epoch:216, train acc:0.5766666666666667, test acc:0.4531 ===\n",
      "train loss:1.6549024515229376\n",
      "train loss:1.4765841662708232\n",
      "train loss:1.5609324067857173\n",
      "=== epoch:217, train acc:0.5933333333333334, test acc:0.4577 ===\n",
      "train loss:1.5611425740928928\n",
      "train loss:1.440627766129065\n",
      "train loss:1.4279476546994585\n",
      "=== epoch:218, train acc:0.5933333333333334, test acc:0.4625 ===\n",
      "train loss:1.4097976444325806\n",
      "train loss:1.5129398868451662\n",
      "train loss:1.4489669594801269\n",
      "=== epoch:219, train acc:0.59, test acc:0.4603 ===\n",
      "train loss:1.5236740400096627\n",
      "train loss:1.478262755580035\n",
      "train loss:1.3785319568172205\n",
      "=== epoch:220, train acc:0.6133333333333333, test acc:0.4647 ===\n",
      "train loss:1.5515738783149013\n",
      "train loss:1.5027505880381191\n",
      "train loss:1.4357880522746567\n",
      "=== epoch:221, train acc:0.6166666666666667, test acc:0.4682 ===\n",
      "train loss:1.360793324150909\n",
      "train loss:1.5403633616699666\n",
      "train loss:1.4555336680638207\n",
      "=== epoch:222, train acc:0.6233333333333333, test acc:0.4721 ===\n",
      "train loss:1.5005672775715706\n",
      "train loss:1.5572103069008654\n",
      "train loss:1.5121000517324048\n",
      "=== epoch:223, train acc:0.6233333333333333, test acc:0.4746 ===\n",
      "train loss:1.4095403143875995\n",
      "train loss:1.4791864249846887\n",
      "train loss:1.4140892275765813\n",
      "=== epoch:224, train acc:0.63, test acc:0.4783 ===\n",
      "train loss:1.408695718781333\n",
      "train loss:1.3687653621776332\n",
      "train loss:1.470224044990914\n",
      "=== epoch:225, train acc:0.63, test acc:0.483 ===\n",
      "train loss:1.392437400721289\n",
      "train loss:1.3890715036421164\n",
      "train loss:1.4247755100729769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:226, train acc:0.6333333333333333, test acc:0.4847 ===\n",
      "train loss:1.4307869106153888\n",
      "train loss:1.412739597220435\n",
      "train loss:1.425179163871055\n",
      "=== epoch:227, train acc:0.6466666666666666, test acc:0.484 ===\n",
      "train loss:1.342627829944409\n",
      "train loss:1.3795060235890277\n",
      "train loss:1.387115190436886\n",
      "=== epoch:228, train acc:0.64, test acc:0.4842 ===\n",
      "train loss:1.5400078133523392\n",
      "train loss:1.3276525071535155\n",
      "train loss:1.3099138163020136\n",
      "=== epoch:229, train acc:0.6333333333333333, test acc:0.4833 ===\n",
      "train loss:1.39470030516806\n",
      "train loss:1.303660638676393\n",
      "train loss:1.3494914590353242\n",
      "=== epoch:230, train acc:0.6233333333333333, test acc:0.4808 ===\n",
      "train loss:1.4296015608673733\n",
      "train loss:1.4200808250202706\n",
      "train loss:1.3552274286677022\n",
      "=== epoch:231, train acc:0.6233333333333333, test acc:0.4836 ===\n",
      "train loss:1.3777803727869529\n",
      "train loss:1.3310490297339728\n",
      "train loss:1.3721660415308983\n",
      "=== epoch:232, train acc:0.64, test acc:0.484 ===\n",
      "train loss:1.3076202475724132\n",
      "train loss:1.480537901270027\n",
      "train loss:1.2167399255057165\n",
      "=== epoch:233, train acc:0.6366666666666667, test acc:0.488 ===\n",
      "train loss:1.4143580123748083\n",
      "train loss:1.4587560970253026\n",
      "train loss:1.2730283931918955\n",
      "=== epoch:234, train acc:0.6333333333333333, test acc:0.4909 ===\n",
      "train loss:1.395307147328952\n",
      "train loss:1.3568681675257104\n",
      "train loss:1.447200555056452\n",
      "=== epoch:235, train acc:0.6433333333333333, test acc:0.4966 ===\n",
      "train loss:1.3898453046164332\n",
      "train loss:1.282006214914011\n",
      "train loss:1.384894696262474\n",
      "=== epoch:236, train acc:0.6466666666666666, test acc:0.5018 ===\n",
      "train loss:1.3511841993031655\n",
      "train loss:1.282378422707493\n",
      "train loss:1.3773660677743487\n",
      "=== epoch:237, train acc:0.6633333333333333, test acc:0.5037 ===\n",
      "train loss:1.3648820310936358\n",
      "train loss:1.3343025213897874\n",
      "train loss:1.4231607828236945\n",
      "=== epoch:238, train acc:0.66, test acc:0.5009 ===\n",
      "train loss:1.377633291515156\n",
      "train loss:1.3918899131470954\n",
      "train loss:1.3325634588831712\n",
      "=== epoch:239, train acc:0.6666666666666666, test acc:0.4986 ===\n",
      "train loss:1.323955490800543\n",
      "train loss:1.2753069482912693\n",
      "train loss:1.2336721622148523\n",
      "=== epoch:240, train acc:0.6466666666666666, test acc:0.4977 ===\n",
      "train loss:1.3397189704872405\n",
      "train loss:1.23453830614808\n",
      "train loss:1.3687206385440405\n",
      "=== epoch:241, train acc:0.67, test acc:0.501 ===\n",
      "train loss:1.3660645136978138\n",
      "train loss:1.3062377529436486\n",
      "train loss:1.3680514691675418\n",
      "=== epoch:242, train acc:0.67, test acc:0.5066 ===\n",
      "train loss:1.3068700107141467\n",
      "train loss:1.2750256956090547\n",
      "train loss:1.3267418308855323\n",
      "=== epoch:243, train acc:0.6633333333333333, test acc:0.5131 ===\n",
      "train loss:1.2875257281893517\n",
      "train loss:1.2599273922155574\n",
      "train loss:1.2087779809609032\n",
      "=== epoch:244, train acc:0.67, test acc:0.5133 ===\n",
      "train loss:1.2703398105742345\n",
      "train loss:1.3264624205089826\n",
      "train loss:1.3106062229710895\n",
      "=== epoch:245, train acc:0.6666666666666666, test acc:0.5151 ===\n",
      "train loss:1.3829411020933449\n",
      "train loss:1.3197775089949273\n",
      "train loss:1.2863482462807425\n",
      "=== epoch:246, train acc:0.6766666666666666, test acc:0.5155 ===\n",
      "train loss:1.289406720759482\n",
      "train loss:1.300318351019194\n",
      "train loss:1.2948588589197791\n",
      "=== epoch:247, train acc:0.6633333333333333, test acc:0.5129 ===\n",
      "train loss:1.2831490944475399\n",
      "train loss:1.3113815182088575\n",
      "train loss:1.2440895408383599\n",
      "=== epoch:248, train acc:0.6866666666666666, test acc:0.5152 ===\n",
      "train loss:1.1493665493476828\n",
      "train loss:1.1480175149629017\n",
      "train loss:1.3441199297029356\n",
      "=== epoch:249, train acc:0.68, test acc:0.5134 ===\n",
      "train loss:1.2782279160828665\n",
      "train loss:1.290623060079785\n",
      "train loss:1.1912179003127294\n",
      "=== epoch:250, train acc:0.68, test acc:0.5113 ===\n",
      "train loss:1.3739748470601594\n",
      "train loss:1.2148858261055968\n",
      "train loss:1.4105983732220262\n",
      "=== epoch:251, train acc:0.6733333333333333, test acc:0.5126 ===\n",
      "train loss:1.2663091727714733\n",
      "train loss:1.2123074553518827\n",
      "train loss:1.2379903832689532\n",
      "=== epoch:252, train acc:0.67, test acc:0.5099 ===\n",
      "train loss:1.1667299258399124\n",
      "train loss:1.051552292690788\n",
      "train loss:1.3054380861590604\n",
      "=== epoch:253, train acc:0.6766666666666666, test acc:0.5099 ===\n",
      "train loss:1.1847887215372115\n",
      "train loss:1.1789838895714604\n",
      "train loss:1.197489598043285\n",
      "=== epoch:254, train acc:0.6766666666666666, test acc:0.5162 ===\n",
      "train loss:1.256406010419751\n",
      "train loss:1.3050699554739944\n",
      "train loss:1.1859921630093515\n",
      "=== epoch:255, train acc:0.68, test acc:0.5194 ===\n",
      "train loss:1.2949657611768912\n",
      "train loss:1.2298701770578762\n",
      "train loss:1.156276298907636\n",
      "=== epoch:256, train acc:0.6833333333333333, test acc:0.5197 ===\n",
      "train loss:1.1171013185742433\n",
      "train loss:1.1094699569638102\n",
      "train loss:1.2896005927437713\n",
      "=== epoch:257, train acc:0.6833333333333333, test acc:0.5195 ===\n",
      "train loss:1.2894524814852282\n",
      "train loss:1.1830205682476354\n",
      "train loss:1.1269799942898617\n",
      "=== epoch:258, train acc:0.6966666666666667, test acc:0.5238 ===\n",
      "train loss:1.2048072083367058\n",
      "train loss:1.1504991608702488\n",
      "train loss:1.2101959537988374\n",
      "=== epoch:259, train acc:0.6933333333333334, test acc:0.5266 ===\n",
      "train loss:1.1608528700922351\n",
      "train loss:0.9987273051237776\n",
      "train loss:1.1930024683410179\n",
      "=== epoch:260, train acc:0.6933333333333334, test acc:0.5262 ===\n",
      "train loss:1.3086131860058163\n",
      "train loss:1.2750144947541415\n",
      "train loss:1.1990078737774923\n",
      "=== epoch:261, train acc:0.6933333333333334, test acc:0.532 ===\n",
      "train loss:1.2806916797454917\n",
      "train loss:1.287164605944156\n",
      "train loss:1.2594597922961541\n",
      "=== epoch:262, train acc:0.7, test acc:0.5316 ===\n",
      "train loss:1.12173598042125\n",
      "train loss:1.1772221170268036\n",
      "train loss:1.2823330436159779\n",
      "=== epoch:263, train acc:0.6933333333333334, test acc:0.5285 ===\n",
      "train loss:1.1504234447308246\n",
      "train loss:1.1515308635543424\n",
      "train loss:1.2379668920068658\n",
      "=== epoch:264, train acc:0.7033333333333334, test acc:0.5329 ===\n",
      "train loss:1.1587759461190554\n",
      "train loss:1.199519083015128\n",
      "train loss:1.261400140743644\n",
      "=== epoch:265, train acc:0.7, test acc:0.5374 ===\n",
      "train loss:1.1449085170334063\n",
      "train loss:1.1044323506052924\n",
      "train loss:1.2157823282025733\n",
      "=== epoch:266, train acc:0.7033333333333334, test acc:0.5394 ===\n",
      "train loss:0.9238916436309412\n",
      "train loss:1.2259709985904883\n",
      "train loss:1.087927927674946\n",
      "=== epoch:267, train acc:0.7166666666666667, test acc:0.5392 ===\n",
      "train loss:1.1447343858532877\n",
      "train loss:1.0423592159946804\n",
      "train loss:1.1328863969792387\n",
      "=== epoch:268, train acc:0.7166666666666667, test acc:0.5406 ===\n",
      "train loss:1.226077992759801\n",
      "train loss:1.1594031191085625\n",
      "train loss:1.0259491611484348\n",
      "=== epoch:269, train acc:0.7133333333333334, test acc:0.5415 ===\n",
      "train loss:1.0041519773409557\n",
      "train loss:1.149005785689356\n",
      "train loss:1.122501293846232\n",
      "=== epoch:270, train acc:0.7133333333333334, test acc:0.5461 ===\n",
      "train loss:1.0858876430974778\n",
      "train loss:1.0689965963996446\n",
      "train loss:1.1090256643936514\n",
      "=== epoch:271, train acc:0.72, test acc:0.5468 ===\n",
      "train loss:1.096143068985819\n",
      "train loss:1.050749684175262\n",
      "train loss:1.0209632921096228\n",
      "=== epoch:272, train acc:0.72, test acc:0.5462 ===\n",
      "train loss:1.119354742237573\n",
      "train loss:1.208361808362079\n",
      "train loss:1.1140377080252062\n",
      "=== epoch:273, train acc:0.7166666666666667, test acc:0.5508 ===\n",
      "train loss:1.1055407018918386\n",
      "train loss:1.0539810224044714\n",
      "train loss:1.1743838556923212\n",
      "=== epoch:274, train acc:0.72, test acc:0.5517 ===\n",
      "train loss:1.1634199154928777\n",
      "train loss:1.1235830243782956\n",
      "train loss:1.171423762301157\n",
      "=== epoch:275, train acc:0.72, test acc:0.5537 ===\n",
      "train loss:0.9154486017076807\n",
      "train loss:1.0334965627553339\n",
      "train loss:1.0127457271654061\n",
      "=== epoch:276, train acc:0.7166666666666667, test acc:0.5544 ===\n",
      "train loss:1.1245167823872635\n",
      "train loss:1.0789208402515043\n",
      "train loss:1.1152711927706165\n",
      "=== epoch:277, train acc:0.7266666666666667, test acc:0.5555 ===\n",
      "train loss:0.9445440491698814\n",
      "train loss:1.0339623813466974\n",
      "train loss:1.026565710730475\n",
      "=== epoch:278, train acc:0.7166666666666667, test acc:0.5539 ===\n",
      "train loss:1.0950666349137537\n",
      "train loss:1.0282613482679104\n",
      "train loss:1.0354127136344686\n",
      "=== epoch:279, train acc:0.72, test acc:0.5547 ===\n",
      "train loss:1.0454076464370647\n",
      "train loss:1.041710597814838\n",
      "train loss:1.0593452260146587\n",
      "=== epoch:280, train acc:0.72, test acc:0.5577 ===\n",
      "train loss:1.059512602149833\n",
      "train loss:1.0911168328770984\n",
      "train loss:1.0099352391434577\n",
      "=== epoch:281, train acc:0.7233333333333334, test acc:0.5586 ===\n",
      "train loss:0.9635453001102989\n",
      "train loss:1.0340303265812927\n",
      "train loss:1.0931700197880339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:282, train acc:0.7233333333333334, test acc:0.5538 ===\n",
      "train loss:0.9817819201650227\n",
      "train loss:0.9171121830926825\n",
      "train loss:0.975774428195633\n",
      "=== epoch:283, train acc:0.7366666666666667, test acc:0.5596 ===\n",
      "train loss:0.9889506960417777\n",
      "train loss:0.9778966678816599\n",
      "train loss:1.02354987069359\n",
      "=== epoch:284, train acc:0.74, test acc:0.5622 ===\n",
      "train loss:1.0979240957295422\n",
      "train loss:0.99028644575363\n",
      "train loss:1.0538897885594494\n",
      "=== epoch:285, train acc:0.7533333333333333, test acc:0.565 ===\n",
      "train loss:0.9204232877637343\n",
      "train loss:1.0565894859222607\n",
      "train loss:0.9832433554443049\n",
      "=== epoch:286, train acc:0.75, test acc:0.5729 ===\n",
      "train loss:0.9792481029704231\n",
      "train loss:1.0608392611833728\n",
      "train loss:0.9964798597067702\n",
      "=== epoch:287, train acc:0.7433333333333333, test acc:0.5717 ===\n",
      "train loss:1.1786860812589088\n",
      "train loss:0.9328791994179335\n",
      "train loss:0.9944815805440421\n",
      "=== epoch:288, train acc:0.76, test acc:0.5782 ===\n",
      "train loss:0.8791898938869288\n",
      "train loss:1.0937508724232765\n",
      "train loss:0.9869545744032087\n",
      "=== epoch:289, train acc:0.7566666666666667, test acc:0.5777 ===\n",
      "train loss:0.9025719939677012\n",
      "train loss:0.891186442255648\n",
      "train loss:0.8822508621963816\n",
      "=== epoch:290, train acc:0.7633333333333333, test acc:0.5782 ===\n",
      "train loss:0.945348279011721\n",
      "train loss:0.9273343982602584\n",
      "train loss:1.028216151727114\n",
      "=== epoch:291, train acc:0.7633333333333333, test acc:0.5799 ===\n",
      "train loss:0.8405957876116277\n",
      "train loss:0.9434784262145348\n",
      "train loss:0.9927742053048364\n",
      "=== epoch:292, train acc:0.76, test acc:0.5775 ===\n",
      "train loss:0.9914338722422319\n",
      "train loss:1.0393977537146022\n",
      "train loss:0.9295271348410311\n",
      "=== epoch:293, train acc:0.76, test acc:0.5821 ===\n",
      "train loss:0.9069512067994526\n",
      "train loss:0.9734935380178865\n",
      "train loss:1.1054995035991584\n",
      "=== epoch:294, train acc:0.7666666666666667, test acc:0.5854 ===\n",
      "train loss:0.9454163051808809\n",
      "train loss:0.8295439150751918\n",
      "train loss:0.8951722774012734\n",
      "=== epoch:295, train acc:0.7633333333333333, test acc:0.5836 ===\n",
      "train loss:0.9226129614943129\n",
      "train loss:0.8683676165896476\n",
      "train loss:0.9934431601164883\n",
      "=== epoch:296, train acc:0.7666666666666667, test acc:0.5846 ===\n",
      "train loss:0.889499063245175\n",
      "train loss:0.8135002104244506\n",
      "train loss:0.9160823212638735\n",
      "=== epoch:297, train acc:0.7633333333333333, test acc:0.5881 ===\n",
      "train loss:0.8114856880916699\n",
      "train loss:0.8860164620905115\n",
      "train loss:1.0116434129563003\n",
      "=== epoch:298, train acc:0.7533333333333333, test acc:0.5858 ===\n",
      "train loss:0.9658995258653066\n",
      "train loss:0.8524025958655044\n",
      "train loss:0.9365635898690017\n",
      "=== epoch:299, train acc:0.7666666666666667, test acc:0.5863 ===\n",
      "train loss:0.9966809968693653\n",
      "train loss:0.9309089065182115\n",
      "train loss:0.8529745941548174\n",
      "=== epoch:300, train acc:0.7633333333333333, test acc:0.5866 ===\n",
      "train loss:0.9487745307205574\n",
      "train loss:0.9364031683177145\n",
      "train loss:0.8653373791742607\n",
      "=== epoch:301, train acc:0.7633333333333333, test acc:0.5875 ===\n",
      "train loss:0.9733387582046282\n",
      "train loss:0.9109991159422386\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5876\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_learning_github.dataset.mnist import load_mnist\n",
    "from deep_learning_github.common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from deep_learning_github.common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
    "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1b77a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 11]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x = np.array([10,11])\n",
    "print(x)\n",
    "print(*x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75d5c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1c9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70995a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff90d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae12395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb95dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2013c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40eba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39158a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051e9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
